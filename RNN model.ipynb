{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718c38cf",
   "metadata": {},
   "source": [
    "## Install the package dependencies before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16ac7530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    number of trajectories in each city\\n    # austin --  train: 43041 test: 6325 \\n    # miami -- train: 55029 test:7971\\n    # pittsburgh -- train: 43544 test: 6361\\n    # dearborn -- train: 24465 test: 3671\\n    # washington-dc -- train: 25744 test: 3829\\n    # palo-alto -- train:  11993 test:1686\\n\\n    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\\n    \\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b472cf2",
   "metadata": {},
   "source": [
    "## Create a Torch.Dataset class for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "091abbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ROOT_PATH = \"./\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "    f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "    inputs = pickle.load(open(f_in, \"rb\"))\n",
    "    inputs = np.asarray(inputs)\n",
    "    \n",
    "    outputs = None\n",
    "    \n",
    "    if split==\"train\":\n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "def get_training_trajectories(normalized=False):\n",
    "    \"\"\" \n",
    "    Get the training trajectories of all cities.\n",
    "    This is useful for if we wish to ignore the city features. \n",
    "    \"\"\"\n",
    "    first_iter = True\n",
    "    cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "    for city in cities:\n",
    "        print(city)\n",
    "        # Assign initial array\n",
    "        if first_iter:\n",
    "            first_iter = False\n",
    "            inputs, outputs = get_city_trajectories(city, split='train', normalized=normalized)\n",
    "            continue\n",
    "        # Get city's trajectories\n",
    "        city_in, city_out = get_city_trajectories(city, split='train', normalized=normalized)\n",
    "        #print(city_in.shape, city_out.shape)\n",
    "        inputs = np.concatenate([inputs, city_in])\n",
    "        outputs = np.concatenate([outputs, city_out])\n",
    "        \n",
    "    #print(inputs.shape)\n",
    "    #print(outputs.shape)\n",
    "    return inputs, outputs\n",
    "\n",
    "\n",
    "def get_test_coords(normalized=False):\n",
    "    \"\"\" Retrieve only the test data, used for submission into the csv set. \"\"\"\n",
    "    first_iter = True\n",
    "    cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "    for city in cities:\n",
    "        # Assign initial array\n",
    "        if first_iter:\n",
    "            first_iter = False\n",
    "            inputs, outputs = get_city_trajectories(city, split='test', normalized=normalized)\n",
    "            continue\n",
    "        # Get city's trajectories\n",
    "        city_in, outputs = get_city_trajectories(city, split='test', normalized=normalized)\n",
    "        inputs = np.concatenate([inputs, city_in])\n",
    "    return inputs\n",
    "\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "\n",
    "        self.inputs, self.outputs = get_city_trajectories(city=city, split=split, normalized=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = (self.inputs[idx], self.outputs[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "class ArgoverseFullDataset(Dataset):\n",
    "    \"\"\" Dataset class for Argoverse. Uses inputs from all cities in contrast to ArgoverseDataset's single city. \"\"\"\n",
    "    def __init__(self, transform=None):\n",
    "        super(ArgoverseFullDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "\n",
    "        self.inputs, self.outputs = get_training_trajectories()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = (self.inputs[idx], self.outputs[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "    \n",
    "    \n",
    "# intialize a dataset\n",
    "city = 'palo-alto' \n",
    "split = 'train'\n",
    "train_dataset  = ArgoverseDataset(city = city, split = split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9875f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austin\n",
      "miami\n",
      "pittsburgh\n",
      "dearborn\n",
      "washington-dc\n",
      "palo-alto\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ArgoverseFullDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6845cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11993, 50, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_city_trajectories()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "333b2fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29843, 50, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_test_coords().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058453cc",
   "metadata": {},
   "source": [
    "## Create a DataLoader class for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5c14f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 500  # batch size \n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80b5e4",
   "metadata": {},
   "source": [
    "## Sample a batch of data and visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c6507c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nex_inp = 0       \\nfor i_batch, sample_batch in enumerate(train_loader):\\n    inp, out = sample_batch\\n    ex_inp = inp\\n    \"\"\"\\n    TODO:\\n      implement your Deep learning model\\n      implement training routine\\n    \"\"\"\\n    show_sample_batch(sample_batch)\\n    break\\n'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "def show_sample_batch(sample_batch):\n",
    "    \"\"\"visualize the trajectory for a batch of samples\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    agent_sz = inp.size(1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        #axs[i].xaxis.set_ticks([]) # These will now show ticks\n",
    "        #axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i,:,0], inp[i,:,1])\n",
    "        axs[i].scatter(out[i,:,0], out[i,:,1])\n",
    "# This doesn't display well with batch sizes over like 10\n",
    "'''\n",
    "ex_inp = 0       \n",
    "for i_batch, sample_batch in enumerate(train_loader):\n",
    "    inp, out = sample_batch\n",
    "    ex_inp = inp\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      implement your Deep learning model\n",
    "      implement training routine\n",
    "    \"\"\"\n",
    "    show_sample_batch(sample_batch)\n",
    "    break\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f384a90",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# MLP design\n",
    "\n",
    "In this Multilayer perceptron model, I will attempt to use the previous 50 steps to predict the next step 60 times. This contrasts the MLP model that predicts the next 60 steps given 50 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "84bab42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation(data):\n",
    "    \"\"\" \n",
    "    This transformation will take a \n",
    "    1x50x2 input tensor and 1x60x2 output tensor \n",
    "    to create a 60x50x2 input tensor and a 1x60x2 output tensor.\n",
    "    This is done so that the MLP will be training on only \"1 step\" predictions\n",
    "    by using the previous predictions to make a more accurate prediction.\n",
    "    This should help with the problem that MLP 60 step predictions are not related by any matter.\n",
    "    \"\"\"\n",
    "    inputs = data[0]\n",
    "    outputs = data[1]\n",
    "    \n",
    "    new_inputs = []\n",
    "    # Create new input tensors\n",
    "    for i in range(60):\n",
    "        # Clip portion of the inputs in order to concatenate output coordinates\n",
    "        input_clip = inputs[i:]\n",
    "        output_clip = outputs[max(0, i - 50):i]\n",
    "        \n",
    "\n",
    "        # Attach the coordinates together\n",
    "        new_input = np.concatenate([\n",
    "            input_clip,\n",
    "            output_clip\n",
    "        ])\n",
    "        \n",
    "        #print(i, new_input.shape)\n",
    "        new_inputs.append(new_input)\n",
    "        \n",
    "    # Output is just the coordinate at given index\n",
    "    outputs = torch.flatten(torch.tensor(outputs), start_dim = 0, end_dim=0)\n",
    "    return (torch.tensor(new_inputs), outputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "357d786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f36017e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"The RNN model.\"\"\"\n",
    "    def __init__(self, rnn_layer, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        # RNN layer for processing input and giving hidden state\n",
    "        self.rnn = rnn_layer\n",
    "        # Input size\n",
    "        self.input_size = self.rnn.input_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        self.layers = self.rnn.num_layers\n",
    "        self.linear = nn.Linear(self.num_hiddens, self.input_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        \"\"\"\n",
    "        Forward pass of inputs to the rnn. \n",
    "        Inputs should be an input tensor in the shape \n",
    "        (batch_size, sequence_length (50), input_size of each coordinate (2 minimum))\n",
    "        State should be a tuple containing hidden state and candidate memory\n",
    "        (num_layers, batch_size, num_hiddens) for both\n",
    "        \"\"\"\n",
    "        X = inputs.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # The fully connected layer will first change the shape of `Y` to\n",
    "        # (`num_steps` * `batch_size`, `num_hiddens`). Its output shape is\n",
    "        # (`num_steps` * `batch_size`, `vocab_size`).\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, batch_size=1, device=None):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # `nn.GRU` takes a tensor as hidden state\n",
    "            return  torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens),\n",
    "                                device=device)\n",
    "        else:\n",
    "            # `nn.LSTM` takes a tuple of hidden states, one for hidden state and one for candidate\n",
    "            # Shape is 10 * * hidden_size \n",
    "            return (\n",
    "                torch.zeros((self.rnn.num_layers, batch_size, self.num_hiddens)),# device=device),\n",
    "                torch.zeros((self.rnn.num_layers, batch_size, self.num_hiddens))#, device=device)\n",
    "            )\n",
    "    \n",
    "    def get_input_size():\n",
    "        return self.input_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "afe3e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def train_epoch_ch8(net, train_iter, loss, updater, device):\n",
    "    \"\"\"Train a net within one epoch (defined in Chapter 8).\"\"\"\n",
    "    state = None\n",
    "    batch_losses = []\n",
    "    input_size = net.input_size\n",
    "    batch_count = 0\n",
    "    for X, Y in train_iter:\n",
    "        state = None\n",
    "        l = 0\n",
    "        batch_size = X.shape[0]\n",
    "        if state is None:\n",
    "            # Initialize `state` when either it is the first iteration or\n",
    "            # using random sampling\n",
    "            state = net.begin_state(batch_size=batch_size, device=device)\n",
    "        else:\n",
    "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "                # `state` is a tensor for `nn.GRU`\n",
    "                state.detach_()\n",
    "            else:\n",
    "                # `state` is a tuple of tensors for `nn.LSTM` and\n",
    "                # for our custom scratch implementation\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        # Reshape outputs to match prediction\n",
    "        y = Y.float()#.reshape((-1, 2)).float()#.T.reshape(-1)\n",
    "        # Transform X to take the first 49 inputs\n",
    "        last_X = X[:, -1:, : ].float()\n",
    "        X = X[:, :-1, :].float()\n",
    "        #print(X.shape)\n",
    "        #print(Y.shape)\n",
    "        #print(last_X.shape)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Forward pass x to warm up\n",
    "        y_hat, state = net(X, state)\n",
    "        \n",
    "        \n",
    "        # Get last prediction as it is an output we need\n",
    "        y_hat, state = net(last_X, state) # Should be form (batch, 1, 2)\n",
    "        y_hat = y_hat.reshape((batch_size, 1, input_size))\n",
    "        prev_in = y_hat\n",
    "        # Build prediction tensor\n",
    "        for i in range(59):\n",
    "            # Predict output\n",
    "            y_hat_new, state = net(prev_in, state)\n",
    "            y_hat_new = y_hat_new.reshape((batch_size, 1, input_size)) \n",
    "            \n",
    "            # Concatenate to tensor of predictions\n",
    "            y_hat = torch.cat([y_hat, y_hat_new.reshape((batch_size, 1, input_size))], dim=1)\n",
    "            #print('y_hat', y_hat.shape)\n",
    "            #print('y_hat_new', y_hat_new.shape)\n",
    "            #print('y shape', y.shape)\n",
    "            # Output becomes input\n",
    "            prev_in = y_hat_new\n",
    "            \n",
    "        # Calculate loss\n",
    "        l = loss(y_hat.float(), y)\n",
    "        # Add for stats\n",
    "        batch_losses.append(l.item())\n",
    "        \n",
    "        # Backpropagate the loss\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            #grad_clipping(net, 1)\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            #grad_clipping(net, 1)\n",
    "            # Since the `mean` function has been invoked\n",
    "            updater(batch_size=1)\n",
    "        #metric.add(l * y.numel(), y.numel())\n",
    "        \n",
    "        batch_count += 1\n",
    "        if batch_count % 50 == 0:\n",
    "            print('batch', batch_count, 'loss', l.item())\n",
    "    return np.mean(batch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a7bc505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch8(net, train_iter, vocab, lr, num_epochs, device,\n",
    "              use_random_iter=False):\n",
    "    \"\"\"Train a model (defined in Chapter 8).\"\"\"\n",
    "    loss = nn.MSELoss()\n",
    "    # Optimizer for sgd\n",
    "    updater = torch.optim.SGD(net.parameters(), lr)\n",
    "    # Train and predict\n",
    "    for epoch in range(num_epochs):\n",
    "        loss, time = train_epoch_ch8(\n",
    "            net, train_loader, loss, updater, device, use_random_iter)\n",
    "        print(f'(train) epoch {epoch + 1}, loss {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ca6251e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1 loss 2172444.5\n",
      "batch 2 loss 2104628.5\n",
      "batch 3 loss 1990670.125\n",
      "batch 4 loss 1947581.625\n",
      "batch 5 loss 1795980.25\n",
      "batch 6 loss 1917227.875\n",
      "batch 7 loss 1869495.25\n",
      "batch 8 loss 1660020.875\n",
      "batch 9 loss 1611281.25\n",
      "batch 10 loss 1783558.125\n",
      "batch 11 loss 1620292.0\n",
      "batch 12 loss 1470810.5\n",
      "batch 13 loss 169212674048.0\n",
      "batch 14 loss 126698012672.0\n",
      "batch 15 loss 607428476928.0\n",
      "batch 16 loss 578317058048.0\n",
      "batch 17 loss 547428663296.0\n",
      "batch 18 loss 562202607616.0\n",
      "batch 19 loss 542126276608.0\n",
      "batch 20 loss 538287964160.0\n",
      "batch 21 loss 494526726144.0\n",
      "batch 22 loss 416647675904.0\n",
      "batch 23 loss 202923704320.0\n",
      "batch 24 loss 415971311616.0\n",
      "batch 25 loss 391694548992.0\n",
      "batch 26 loss 371401785344.0\n",
      "batch 27 loss 348490006528.0\n",
      "batch 28 loss 312660983808.0\n",
      "batch 29 loss 117239472128.0\n",
      "batch 30 loss 103816765440.0\n",
      "batch 31 loss 103392288768.0\n",
      "batch 32 loss 110225268736.0\n",
      "batch 33 loss 78811488256.0\n",
      "batch 34 loss 83832733696.0\n",
      "batch 35 loss 44767215616.0\n",
      "batch 36 loss 48355397632.0\n",
      "batch 37 loss 42532892672.0\n",
      "batch 38 loss 42423173120.0\n",
      "batch 39 loss 40173506560.0\n",
      "batch 40 loss 35950018560.0\n",
      "batch 41 loss 34330101760.0\n",
      "batch 42 loss 36052389888.0\n",
      "batch 43 loss 38516047872.0\n",
      "batch 44 loss 39242801152.0\n",
      "batch 45 loss 8963226624.0\n",
      "batch 46 loss 7955436544.0\n",
      "batch 47 loss 7282878976.0\n",
      "batch 48 loss 5375380992.0\n",
      "batch 49 loss 11095547904.0\n",
      "batch 50 loss 62719086592.0\n",
      "batch 51 loss 113777524736.0\n",
      "batch 52 loss 102631768064.0\n",
      "batch 53 loss 97842454528.0\n",
      "batch 54 loss 91214405632.0\n",
      "batch 55 loss 86862110720.0\n",
      "batch 56 loss 81875034112.0\n",
      "batch 57 loss 78450229248.0\n",
      "batch 58 loss 74326712320.0\n",
      "batch 59 loss 70478553088.0\n",
      "batch 60 loss 65957105664.0\n",
      "batch 61 loss 61212925952.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [120]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Train and predict\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m loss, time \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch_ch8\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdater\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(train) epoch, loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [118]\u001b[0m, in \u001b[0;36mtrain_epoch_ch8\u001b[1;34m(net, train_iter, loss, updater, device)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(updater, torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer):\n\u001b[0;32m     63\u001b[0m     updater\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 64\u001b[0m     \u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m#grad_clipping(net, 1)\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     updater\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn_lstm = torch.nn.LSTM(\n",
    "    input_size=2,\n",
    "    hidden_size=256,\n",
    "    num_layers=1,\n",
    "    batch_first=True\n",
    ")\n",
    "net = RNNModel(rnn_lstm)\n",
    "\n",
    "num_epochs, lr = 500, .001\n",
    "loss = nn.MSELoss()\n",
    "# Optimizer for sgd\n",
    "updater = torch.optim.SGD(net.parameters(), lr)\n",
    "device = torch.device('cpu')\n",
    "# Train and predict\n",
    "loss, time = train_epoch_ch8(net, train_loader, loss, updater, device)\n",
    "print(f'(train) epoch, loss {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787370ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X follows the form of (batch, sequence length, then inputs)\n",
    "batch_sz = 10\n",
    "sequence_len = 50 # Always first 50 inputs\n",
    "input_size = 2 # x and y, may contain other features later on\n",
    "X = torch.rand((batch_sz, sequence_len, input_size))\n",
    "\n",
    "#rnn_lstm(X, net.begin_state(batch_sz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6fb8f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3779, 0.8248],\n",
       "        [0.7628, 0.0366],\n",
       "        [0.3324, 0.1314],\n",
       "        [0.8373, 0.5321]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = torch.rand((4, 2))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4264b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3779, 1.8248],\n",
       "        [1.7628, 1.0366],\n",
       "        [1.3324, 1.1314],\n",
       "        [1.8373, 1.5321]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample2 = sample + 1\n",
    "sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "587480e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3779, 0.8248, 0.7628, 0.0366, 0.3324, 0.1314, 0.8373, 0.5321, 1.3779,\n",
       "        1.8248, 1.7628, 1.0366, 1.3324, 1.1314, 1.8373, 1.5321])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat([sample.reshape((1, 4, 2)), sample2.reshape(1, 4, 2)]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba5327f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
