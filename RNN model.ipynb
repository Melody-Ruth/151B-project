{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718c38cf",
   "metadata": {},
   "source": [
    "## Install the package dependencies before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ac7530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    number of trajectories in each city\\n    # austin --  train: 43041 test: 6325 \\n    # miami -- train: 55029 test:7971\\n    # pittsburgh -- train: 43544 test: 6361\\n    # dearborn -- train: 24465 test: 3671\\n    # washington-dc -- train: 25744 test: 3829\\n    # palo-alto -- train:  11993 test:1686\\n\\n    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\\n    \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b472cf2",
   "metadata": {},
   "source": [
    "## Create a Torch.Dataset class for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "091abbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ROOT_PATH = \"./\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "    f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "    inputs = pickle.load(open(f_in, \"rb\"))\n",
    "    inputs = np.asarray(inputs)\n",
    "    \n",
    "    outputs = None\n",
    "    \n",
    "    if split==\"train\":\n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "def get_training_trajectories(normalized=False):\n",
    "    \"\"\" \n",
    "    Get the training trajectories of all cities.\n",
    "    This is useful for if we wish to ignore the city features. \n",
    "    \"\"\"\n",
    "    first_iter = True\n",
    "    cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "    for city in cities:\n",
    "        print(city)\n",
    "        # Assign initial array\n",
    "        if first_iter:\n",
    "            first_iter = False\n",
    "            inputs, outputs = get_city_trajectories(city, split='train', normalized=normalized)\n",
    "            continue\n",
    "        # Get city's trajectories\n",
    "        city_in, city_out = get_city_trajectories(city, split='train', normalized=normalized)\n",
    "        #print(city_in.shape, city_out.shape)\n",
    "        inputs = np.concatenate([inputs, city_in])\n",
    "        outputs = np.concatenate([outputs, city_out])\n",
    "        \n",
    "    #print(inputs.shape)\n",
    "    #print(outputs.shape)\n",
    "    return inputs, outputs\n",
    "\n",
    "\n",
    "def get_test_coords(normalized=False):\n",
    "    \"\"\" Retrieve only the test data, used for submission into the csv set. \"\"\"\n",
    "    first_iter = True\n",
    "    cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "    for city in cities:\n",
    "        # Assign initial array\n",
    "        if first_iter:\n",
    "            first_iter = False\n",
    "            inputs, outputs = get_city_trajectories(city, split='test', normalized=normalized)\n",
    "            continue\n",
    "        # Get city's trajectories\n",
    "        city_in, outputs = get_city_trajectories(city, split='test', normalized=normalized)\n",
    "        inputs = np.concatenate([inputs, city_in])\n",
    "    return inputs\n",
    "\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "\n",
    "        self.inputs, self.outputs = get_city_trajectories(city=city, split=split, normalized=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = (self.inputs[idx], self.outputs[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "class ArgoverseFullDataset(Dataset):\n",
    "    \"\"\" Dataset class for Argoverse. Uses inputs from all cities in contrast to ArgoverseDataset's single city. \"\"\"\n",
    "    def __init__(self, transform=None):\n",
    "        super(ArgoverseFullDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "\n",
    "        self.inputs, self.outputs = get_training_trajectories()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.inputs.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = (self.inputs[idx], self.outputs[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "    \n",
    "    \n",
    "# intialize a dataset\n",
    "city = 'palo-alto' \n",
    "split = 'train'\n",
    "train_dataset  = ArgoverseDataset(city = city, split = split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9875f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austin\n",
      "miami\n",
      "pittsburgh\n",
      "dearborn\n",
      "washington-dc\n",
      "palo-alto\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ArgoverseFullDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058453cc",
   "metadata": {},
   "source": [
    "## Create a DataLoader class for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42bd8b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183434"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_size = int(np.floor(len(train_dataset) * .90))\n",
    "train_set_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96f9d1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20382"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_size = len(train_dataset) - train_set_size\n",
    "test_set_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaa1c079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test and validation set\n",
    "train_data, validation_data = torch.utils.data.random_split(train_dataset, [train_set_size, test_set_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1932e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 5000  # batch size \n",
    "train_set = DataLoader(train_data, batch_size=batch_sz)\n",
    "test_set = DataLoader(validation_data, batch_size=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80b5e4",
   "metadata": {},
   "source": [
    "## Sample a batch of data and visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6507c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nex_inp = 0       \\nfor i_batch, sample_batch in enumerate(train_set):\\n    inp, out = sample_batch\\n    ex_inp = inp\\n    \"\"\"\\n    TODO:\\n      implement your Deep learning model\\n      implement training routine\\n    \"\"\"\\n    show_sample_batch(sample_batch)\\n    break\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "def show_sample_batch(sample_batch):\n",
    "    \"\"\"visualize the trajectory for a batch of samples\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    agent_sz = inp.size(1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        #axs[i].xaxis.set_ticks([]) # These will now show ticks\n",
    "        #axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i,:,0], inp[i,:,1])\n",
    "        axs[i].scatter(out[i,:,0], out[i,:,1])\n",
    "    print('show')\n",
    "    plt.show()\n",
    "# This doesn't display well with batch sizes over like 10\n",
    "'''\n",
    "ex_inp = 0       \n",
    "for i_batch, sample_batch in enumerate(train_set):\n",
    "    inp, out = sample_batch\n",
    "    ex_inp = inp\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      implement your Deep learning model\n",
    "      implement training routine\n",
    "    \"\"\"\n",
    "    show_sample_batch(sample_batch)\n",
    "    break\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f384a90",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# MLP design\n",
    "\n",
    "In this Multilayer perceptron model, I will attempt to use the previous 50 steps to predict the next step 60 times. This contrasts the MLP model that predicts the next 60 steps given 50 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84bab42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation(data):\n",
    "    \"\"\" \n",
    "    This transformation will take a \n",
    "    1x50x2 input tensor and 1x60x2 output tensor \n",
    "    to create a 60x50x2 input tensor and a 1x60x2 output tensor.\n",
    "    This is done so that the MLP will be training on only \"1 step\" predictions\n",
    "    by using the previous predictions to make a more accurate prediction.\n",
    "    This should help with the problem that MLP 60 step predictions are not related by any matter.\n",
    "    \"\"\"\n",
    "    inputs = data[0]\n",
    "    outputs = data[1]\n",
    "    \n",
    "    new_inputs = []\n",
    "    # Create new input tensors\n",
    "    for i in range(60):\n",
    "        # Clip portion of the inputs in order to concatenate output coordinates\n",
    "        input_clip = inputs[i:]\n",
    "        output_clip = outputs[max(0, i - 50):i]\n",
    "        \n",
    "\n",
    "        # Attach the coordinates together\n",
    "        new_input = np.concatenate([\n",
    "            input_clip,\n",
    "            output_clip\n",
    "        ])\n",
    "        \n",
    "        #print(i, new_input.shape)\n",
    "        new_inputs.append(new_input)\n",
    "        \n",
    "    # Output is just the coordinate at given index\n",
    "    outputs = torch.flatten(torch.tensor(outputs), start_dim = 0, end_dim=0)\n",
    "    return (torch.tensor(new_inputs), outputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "357d786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f36017e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"The RNN model.\"\"\"\n",
    "    def __init__(self, rnn_layer, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        # RNN layer for processing input and giving hidden state\n",
    "        self.rnn = rnn_layer\n",
    "        # Input size\n",
    "        self.input_size = self.rnn.input_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        self.layers = self.rnn.num_layers\n",
    "        self.linear = nn.Linear(self.num_hiddens, self.input_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        \"\"\"\n",
    "        Forward pass of inputs to the rnn. \n",
    "        Inputs should be an input tensor in the shape \n",
    "        (batch_size, sequence_length (50), input_size of each coordinate (2 minimum))\n",
    "        State should be a tuple containing hidden state and candidate memory\n",
    "        (num_layers, batch_size, num_hiddens) for both\n",
    "        \"\"\"\n",
    "        X = inputs.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # The fully connected layer will first change the shape of `Y` to\n",
    "        # (`num_steps` * `batch_size`, `num_hiddens`). Its output shape is\n",
    "        # (`num_steps` * `batch_size`, `vocab_size`).\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, batch_size, device):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # `nn.GRU` takes a tensor as hidden state\n",
    "            return  torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens),\n",
    "                                device=device)\n",
    "        else:\n",
    "            # `nn.LSTM` takes a tuple of hidden states, one for hidden state and one for candidate\n",
    "            # Shape is 10 * * hidden_size \n",
    "            return (\n",
    "                [torch.zeros((self.rnn.num_layers, batch_size, self.num_hiddens), device=device),\n",
    "                torch.zeros((self.rnn.num_layers, batch_size, self.num_hiddens), device=device)]\n",
    "            )\n",
    "    \n",
    "    def get_input_size():\n",
    "        return self.input_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df51a086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(net, test_iter, loss_fn, device):\n",
    "    state = None\n",
    "    input_size = net.input_size\n",
    "    losses = []\n",
    "    for X, y in test_iter:\n",
    "        batch_size = X.shape[0]\n",
    "        # ReInitialize state\n",
    "        state = net.begin_state(X.shape[0], device)\n",
    "        # Warm up states\n",
    "        last_X = X[:, -1:, :].detach().to(device)\n",
    "        X = X[:, :-1, :].detach().float()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        Y_hat, state = net(X, state)\n",
    "        # Start grading loss\n",
    "        predictions, state = net(last_X, state)\n",
    "        predictions = predictions.reshape((batch_size, 1, input_size))\n",
    "        prev_in = predictions\n",
    "        #print(predictions.shape)\n",
    "        for i in range(59): \n",
    "            # Keep passing in the arg\n",
    "            # Predict output\n",
    "            y_hat_new, state = net(prev_in, state)\n",
    "            #print(y_hat_new.shape)\n",
    "            y_hat_new = y_hat_new.reshape((batch_size, 1, input_size)) \n",
    "            #print(y_hat_new.shape)\n",
    "            # Concatenate to tensor of predictions\n",
    "            predictions = torch.cat([predictions, y_hat_new.reshape((batch_size, 1, input_size))], dim=1)\n",
    "            # Previous out becomes new in\n",
    "            #print(predictions.shape)\n",
    "            prev_in = y_hat_new\n",
    "            \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(y, predictions)\n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afe3e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def train_epoch_ch8(net, train_iter, loss, updater, device):\n",
    "    \"\"\"Train a net within one epoch (defined in Chapter 8).\"\"\"\n",
    "    state = None\n",
    "    batch_losses = []\n",
    "    input_size = net.input_size\n",
    "    batch_count = 0\n",
    "    for X, Y in train_iter:\n",
    "        state = None\n",
    "        l = 0\n",
    "        batch_size = X.shape[0]\n",
    "        if state is None:\n",
    "            # Initialize `state` when either it is the first iteration or\n",
    "            # using random sampling\n",
    "            state = net.begin_state(batch_size, device)\n",
    "            #print(state)\n",
    "        else:\n",
    "            \n",
    "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "                # `state` is a tensor for `nn.GRU`\n",
    "                state.detach_()\n",
    "            else:\n",
    "                # `state` is a tuple of tensors for `nn.LSTM` and\n",
    "                # for our custom scratch implementation\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "            #print(state[0].shape, batch_size)\n",
    "            if state[0].shape[1] != batch_size:\n",
    "                state = (state[0][:, :batch_size, :],\n",
    "                        state[1][:, :batch_size, :])\n",
    "        # Reshape outputs to match prediction\n",
    "        y = Y.float()#.reshape((-1, 2)).float()#.T.reshape(-1)\n",
    "        # Transform X to take the first 49 inputs\n",
    "        last_X = X[:, -1:, : ].to(device)\n",
    "        X = X[:, :-1, :].float()\n",
    "        #print(X.shape)\n",
    "        #print(Y.shape)\n",
    "        #print(last_X.shape)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Ensure state is to device\n",
    "        # Forward pass x to warm up\n",
    "        y_hat, state = net(X, (state[0], state[1]))\n",
    "        \n",
    "        \n",
    "        # Get last prediction as it is an output we need\n",
    "        y_hat, state = net(last_X, state) # Should be form (batch, 1, 2)\n",
    "        y_hat = y_hat.reshape((batch_size, 1, input_size))\n",
    "        prev_in = y_hat\n",
    "        # Build prediction tensor\n",
    "        for i in range(59):\n",
    "            # Predict output\n",
    "            y_hat_new, state = net(prev_in, state)\n",
    "            y_hat_new = y_hat_new.reshape((batch_size, 1, input_size)) \n",
    "            \n",
    "            # Concatenate to tensor of predictions\n",
    "            y_hat = torch.cat([y_hat, y_hat_new.reshape((batch_size, 1, input_size))], dim=1)\n",
    "            #print('y_hat', y_hat.shape)\n",
    "            #print('y_hat_new', y_hat_new.shape)\n",
    "            #print('y shape', y.shape)\n",
    "            # Output becomes input\n",
    "            prev_in = y_hat_new\n",
    "            \n",
    "        # Calculate loss\n",
    "        l = loss(y_hat.float(), y)\n",
    "        # Add for stats\n",
    "        batch_losses.append(l.item())\n",
    "        \n",
    "        # Backpropagate the loss\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            #grad_clipping(net, 1)\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            #grad_clipping(net, 1)\n",
    "            # Since the `mean` function has been invoked\n",
    "            updater(batch_size=1)\n",
    "        #metric.add(l * y.numel(), y.numel())\n",
    "        \n",
    "        batch_count += 1\n",
    "        #if batch_count % 1 == 0:\n",
    "            #print('batch', batch_count, 'loss', l.item())\n",
    "    return np.mean(batch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2f2146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, pred_iter, device):\n",
    "    # Get \n",
    "    state = None\n",
    "    input_size = net.input_size\n",
    "    all_predictions = None\n",
    "    for X, y in pred_iter:\n",
    "        batch_size = X.shape[0]\n",
    "        print(state[0].shape, X.shape)\n",
    "        # ReInitialize state or trim state\n",
    "        if state == None:\n",
    "            state = net.begin_state(batch_size=batch_size, device=device)\n",
    "            state[0] = state[0].to(device)\n",
    "            state[1] = state[1].to(device)\n",
    "        elif state[0].shape[1] != batch_size:\n",
    "            state = (state[0][:, :batch_size, :],\n",
    "                    state[1][:, :batch_size, :])\n",
    "        # Warm up states\n",
    "        last_X = X[:, -1:, :].detach().to(device)\n",
    "        X = X[:, :-1, :].detach().float()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        Y_hat, state = net(X, state)\n",
    "        # Start grading loss\n",
    "        predictions, state = net(last_X, state)\n",
    "        predictions = predictions.reshape((batch_size, 1, input_size))\n",
    "        prev_in = predictions\n",
    "        #print(predictions.shape)\n",
    "        for i in range(59): \n",
    "            # Keep passing in the arg\n",
    "            # Predict output\n",
    "            y_hat_new, state = net(prev_in, state)\n",
    "            #print(y_hat_new.shape)\n",
    "            y_hat_new = y_hat_new.reshape((batch_size, 1, input_size)) \n",
    "            #print(y_hat_new.shape)\n",
    "            # Concatenate to tensor of predictions\n",
    "            predictions = torch.cat([predictions, y_hat_new.reshape((batch_size, 1, input_size))], dim=1)\n",
    "            # Previous out becomes new in\n",
    "            #print(predictions.shape)\n",
    "            prev_in = y_hat_new\n",
    "            \n",
    "        # Concatenate predictions\n",
    "        if all_predictions == None:\n",
    "            all_predictions = predictions\n",
    "        else:\n",
    "            all_predictions = torch.concat([all_predictions, predictions])\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7bc505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch8(net, train_iter, test_iter, lr, num_epochs, device,\n",
    "              use_random_iter=False):\n",
    "    \"\"\"Train a model (defined in Chapter 8).\"\"\"\n",
    "    loss = nn.MSELoss()\n",
    "    # Optimizer for sgd\n",
    "    updater = torch.optim.SGD(net.parameters(), lr)\n",
    "    # Train and predict\n",
    "    epoch_train_err = []\n",
    "    epoch_val_err = []\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = train_epoch_ch8(\n",
    "            net, train_iter, loss, updater, device)\n",
    "        val_err = get_loss(net, test_iter, loss, device)\n",
    "        print(f'(train) epoch {epoch + 1}, train err {total_loss}, val err {val_err}')\n",
    "        # Get training error\n",
    "        epoch_val_err.append(val_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10705098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (rnn): LSTM(2, 512, batch_first=True)\n",
       "  (linear): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_lstm = torch.nn.LSTM(\n",
    "    input_size=2,\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    batch_first=True\n",
    ")\n",
    "net = RNNModel(rnn_lstm)\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca6251e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, lr = 100, .00001\n",
    "loss_fn = nn.MSELoss()\n",
    "# Optimizer for sgd\n",
    "#updater = torch.optim.SGD(net.parameters(), lr)\n",
    "device = torch.device('cuda')\n",
    "# Train and predict\n",
    "#loss = train_epoch_ch8(net, train_set, loss_fn, updater, device)\n",
    "#print(f'(train) epoch, loss {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f8a0d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(train) epoch 1, train err 10060114.716216216, val err 8753210.556068666\n",
      "(train) epoch 2, train err 7762870.304054054, val err 6955004.826928768\n",
      "(train) epoch 3, train err 6509417.128378378, val err 6050359.434161817\n",
      "(train) epoch 4, train err 5902137.878378378, val err 5653005.600592541\n",
      "(train) epoch 5, train err 5609017.081081081, val err 5183673.302734093\n",
      "(train) epoch 6, train err 5423720.277027027, val err 5170969.667208303\n",
      "(train) epoch 7, train err 5336500.824324325, val err 4999650.346765022\n",
      "(train) epoch 8, train err 554349773847.8851, val err 286878519165.1898\n",
      "(train) epoch 9, train err 225684560204.1081, val err 328615712421.67377\n",
      "(train) epoch 10, train err 439235973341.4054, val err 633874913456.038\n",
      "(train) epoch 11, train err 989006122897.2972, val err 839728756890.4042\n",
      "(train) epoch 12, train err 1127655580312.2163, val err 736524867730.312\n",
      "(train) epoch 13, train err 10882649310512.432, val err 5604146997504.623\n",
      "(train) epoch 14, train err 6237884503676.541, val err 7210369145326.242\n",
      "(train) epoch 15, train err 7549447335603.892, val err 11160146769316.436\n",
      "(train) epoch 16, train err 7808822740715.243, val err 8002805363745.6455\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3706/2982081343.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_ch8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3706/923016745.py\u001b[0m in \u001b[0;36mtrain_ch8\u001b[0;34m(net, train_iter, test_iter, lr, num_epochs, device, use_random_iter)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mepoch_val_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         total_loss = train_epoch_ch8(\n\u001b[0m\u001b[1;32m     12\u001b[0m             net, train_iter, loss, updater, device)\n\u001b[1;32m     13\u001b[0m         \u001b[0mval_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3706/2949229076.py\u001b[0m in \u001b[0;36mtrain_epoch_ch8\u001b[0;34m(net, train_iter, loss, updater, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbatch_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m#state = None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# scalars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# scalars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_ch8(net, train_set, test_set, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdb168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_loss(net, test_set, nn.MSELoss(), torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b97135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict(net, test_set, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d74c1de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
